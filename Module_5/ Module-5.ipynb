{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee542e2d-fa08-4e25-9592-09a4c9fc22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e37d631-f8e9-4b37-956f-e5762133d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation functions\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s\n",
    "    \n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16e458d1-6d64-4468-9e35-2ba9d1fc19f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "    def __init__(self, eta, n_hidden, D_input, D_hidden, D_output):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.D_input = D_input        # Input dimension\n",
    "        self.D_hidden = D_hidden      # Hidden layer dimension\n",
    "        self.D_output = D_output      # Output dimension\n",
    "\n",
    "    def parameters(self):\n",
    "        W1 = np.random.randn(self.D_hidden, self.D_input) * np.sqrt(1 / self.D_input)\n",
    "        b1 = np.random.randn(self.D_hidden, 1)\n",
    "        W2 = np.random.randn(self.D_output, self.D_hidden) * np.sqrt(1 / self.D_hidden)\n",
    "        b2 = np.random.randn(self.D_output, 1)\n",
    "        return {'W1': W1, 'W2': W2, 'b1': b1, 'b2': b2}\n",
    "\n",
    "    def ForwardPropagation(self, X, params):\n",
    "       Z1 = np.dot(params['W1'], X)+params['b1']\n",
    "       A1 = np.tanh(Z1)\n",
    "       Z2 = np.dot(params['W2'], A1)+params['b2']\n",
    "       y = sigmoid(Z2)  \n",
    "       return y, {'Z1': Z1, 'Z2': Z2, 'A1': A1, 'y': y}\n",
    "        \n",
    "    \n",
    "    def CostEstimate(self, Y, y_hat):\n",
    "        m = Y.shape[1]  # Number of samples\n",
    "        cost = -(1 / m) * np.sum(Y * np.log(y_hat) + (1 - Y) * np.log(1 - y_hat))\n",
    "        return np.squeeze(cost)  # Ensure it's a scalar\n",
    "\n",
    "    def BackPropagation(self, params, cache, X, Y):\n",
    "         m = X.shape[1]\n",
    "    \n",
    "         # First, retrieve W from the dictionary \"parameters\".\n",
    "         W1 = params[\"W1\"]\n",
    "         W2 = params[\"W2\"]\n",
    "    \n",
    "         # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "         A1 = cache[\"A1\"]\n",
    "         y = cache[\"y\"]\n",
    "    \n",
    "    # Backward propagation: calculate partial derivatives denoted as dW1, db1, dW2, db2 for simplicity. \n",
    "         dZ2 = y - Y\n",
    "         dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "         db2 = 1/m * np.sum(dZ2, axis = 1, keepdims = True)\n",
    "         dZ1 = np.dot(W2.T, dZ2) * A1 * (1 - A1)\n",
    "         dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "         db1 = 1/m * np.sum(dZ1, axis = 1, keepdims = True)\n",
    "    \n",
    "         grads = {\"dW1\": dW1,\n",
    "                  \"db1\": db1,\n",
    "                  \"dW2\": dW2,\n",
    "                  \"db2\": db2}\n",
    "    \n",
    "         return grads\n",
    "\n",
    "#grads = backward_propagation(parameters, cache, X, Y)\n",
    "\n",
    "#print(\"dW1 = \" + str(grads[\"dW1\"]))\n",
    "#print(\"db1 = \" + str(grads[\"db1\"]))\n",
    "#print(\"dW2 = \" + str(grads[\"dW2\"]))\n",
    "#print(\"db2 = \" + str(grads[\"db2\"]))\n",
    "\n",
    "    def UpdateParameters(self, params, grads):\n",
    "         W1 = parameters[\"W1\"]\n",
    "         b1 = parameters[\"b1\"]\n",
    "         W2 = parameters[\"W2\"]\n",
    "         b2 = parameters[\"b2\"]\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "       # Retrieve each gradient from the dictionary \"grads\".\n",
    "       ### START CODE HERE ### (~ 4 lines of code)\n",
    "         dW1 = grads[\"dW1\"]\n",
    "         db1 = grads[\"db1\"]\n",
    "         dW2 = grads[\"dW2\"]\n",
    "         db2 = grads[\"db2\"]\n",
    "       ### END CODE HERE ###\n",
    "    \n",
    "       # Update rule for each parameter.\n",
    "       ### START CODE HERE ### (~ 4 lines of code)\n",
    "         W1 = W1 - learning_rate * dW1\n",
    "         b1 = b1 - learning_rate * db1\n",
    "         W2 = W2 - learning_rate * dW2\n",
    "         b2 = b2 - learning_rate * db2\n",
    "       ### END CODE HERE ###\n",
    "    \n",
    "         parameters = {\"W1\": W1,\n",
    "                       \"b1\": b1,\n",
    "                       \"W2\": W2,\n",
    "                       \"b2\": b2}\n",
    "    \n",
    "         return parameters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff30bf9a-d179-4357-b507-1a2c5193689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model\n",
    "D_input = X_train.shape[1]\n",
    "print('input size',input_size)\n",
    "D_hidden = 2\n",
    "D_output = y_train.shape[1]\n",
    "# Training parameters\n",
    "num_epochs = 300 #n_iter\n",
    "learning_rate = 0.01 #eta\n",
    "NK = neural_network(learning_rate, num_epoch, D_input, D_hidden, D_output)\n",
    "\n",
    " \n",
    " # Training loop\n",
    "for epoch in range(num_epochs):\n",
    "        params = parameters(X_train)\n",
    "        # Forward pass\n",
    "        output = NK.ForwardPropagation(X_train.T, params)\n",
    "\n",
    "        # Backward pass and parameter update\n",
    "        gradients = NK.BackPropagation(X_train.T, y_train.T)\n",
    "        mlp.update_parameters(gradients, learning_rate)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = np.mean((outputs - y_train.T) ** 2)\n",
    "        if (epoch + 1) % 300 == 0:\n",
    "            print(f\"Epoch {epoch+1} - Loss: {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
